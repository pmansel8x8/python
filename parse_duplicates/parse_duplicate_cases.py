"""
This python script will scan a CSV file that should contain data about
duplicate cases. The format of the file should be:

"ID","EMAIL_HASH","C","VALUE","CASE_STATUS"
3202517,"+/wenqqXPJeaCGgs2dPlcA==",2,"Unassigned","Not updated"
3202570,"+/wenqqXPJeaCGgs2dPlcA==",2,"Closed","Updated"
...

This data can be generated by the following SQL query:

SELECT
    cases.id,
    b.email_hash,
    b.c,
    w.value,
    decode(nvl(cases.last_activity, cases.open_date), cases.open_date, 'Not updated', 'Updated') case_status
FROM
         (
        SELECT
            a.*,
            COUNT(*)
            OVER(PARTITION BY email_hash) c
        FROM
            (
                SELECT
                    c.id                                       AS caseid,
                    wpjadmin.blob_md5_checksum(f.file_content) AS email_hash
                FROM
                         tenant0099.cases c
                    JOIN tenant0099.attachments a ON ( a.caseid = c.id
                                                       AND a.followupid IS NULL )
                    JOIN tenant0099.files       f ON ( f.id = a.fileid )
                WHERE
                    c.open_date BETWEEN TO_DATE('2021-12-13 00:00:00', 'YYYY-MM-DD HH24:MI:SS') AND TO_DATE('2021-12-15 23:59:59', 'YYYY-MM-DD HH24:MI:SS')
                    AND a.file_name = 'message_source.txt'
            ) a
    ) b
    JOIN tenant0099.cases         cases ON ( b.caseid = cases.id )
    JOIN tenant0099.wpj_picklists w ON ( cases.cf_status.wpj_picklist = w.id )
WHERE
    b.c > 1
ORDER BY
    b.email_hash,
    cases.id;

The duplicate cases will be scanned, and those that have not been updated will
be deleted with one exception: If all duplicate copies of a particular case
have not been worked on, we will keep the oldest copy and delete the other ones. 
"""
import csv
import time
from collections import defaultdict
from rich import print
from rich.live import Live
from rich.panel import Panel
from rich.progress import track
from rich.table import Table


def generate_table(row: list) -> Table:
    """Make a new table."""
    table = Table()
    table.add_column("Hash")
    table.add_column("Case Id")
    table.add_column("# copies")
    table.add_column("Status")

    if len(row) > 0:
        table.add_row(
            f"{row[1]}", f"{row[0]}", f"{row[2]}", "[red]Updated" if row[4] == "Updated" else "[green]Not updated"
        )
    return table


def parse_csv_file(filename: str) -> defaultdict:
    """
    Parse CSV file, and store duplicates in a dictionary where
    the key is the email hash, and the value is a list of CSV records
    that share the same hash
    """
    # ['ID', 'EMAIL_HASH', 'C', 'VALUE', 'CASE_STATUS']

    cases_by_hash = defaultdict(list)
    with open(filename, "r") as csvfile:
        csvreader = csv.reader(csvfile)

        # Skip header
        next(csvreader)

        with Live(generate_table([]), refresh_per_second=100) as live:
            for row in csvreader:
                cases_by_hash[row[1]].append(row)
                live.update(generate_table(row))

                # Remove this if you want speed over fancy display
                time.sleep(0.01)

    return cases_by_hash


def generate_list_of_ids_to_delete(cases_by_hash: defaultdict) -> list:
    """
    For each group of duplicates, check if there are copies that have not
    been worked on. When that happens, check if all copies have not been
    worked on, or only some of them. If all copies have not been worked on,
    keep the oldest copy, and delete all the other ones. If only some
    copies have not been worked on, delete all of them.
    Returns a list of all the IDs to delete.
    """
    cases_to_delete: list = []
    for item in track(cases_by_hash.items(), description="Processing..."):
        email_cases: list = item[1]
        nb_email_cases = len(email_cases)
        nb_email_cases_updated = 0
        nb_email_cases_not_updated = 0

        for email_case in email_cases:
            if email_case[4] == "Updated":
                nb_email_cases_updated += 1
            else:
                nb_email_cases_not_updated += 1

        # All duplicates have been updated. Do not delete
        if nb_email_cases_not_updated == 0:
            continue
        # All duplicates are not updated => delete all but the lowest case id
        elif nb_email_cases_not_updated == nb_email_cases:
            for email_case in email_cases[1:]:
                cases_to_delete.append(email_case[0])
        # All non-updated duplicates should be deleted
        else:
            for email_case in email_cases:
                if email_case[4] == "Not updated":
                    cases_to_delete.append(email_case[0])

        # Remove this if you want speed over fancy display
        time.sleep(0.01)

    print(f"Found {len(cases_to_delete)} cases to delete")
    return cases_to_delete


def build_delete_statement(tenant: str, table: str, cases_to_delete: list) -> str:
    """
    Formats the DELETE statement using the list of case IDs
    that should be deleted.
    """
    cases_id_list = ""
    for caseid in cases_to_delete[:-1]:
        cases_id_list += caseid + ","

    cases_id_list += cases_to_delete[-1]
    return (
        f"[red]DELETE FROM[/red] [blue]{tenant}[/blue].[green]{table}[/green] [red]WHERE ID IN[/red] ({cases_id_list});"
    )


def main():
    tenant = "tenant0099"
    table = "cases"
    csv_filename = "/Users/pansel/JiraTickets/VCC-55487/python/duplicate_cases.csv"
    id_list = generate_list_of_ids_to_delete(parse_csv_file(csv_filename))
    delete_statement = build_delete_statement(tenant, table, id_list)
    print(Panel(delete_statement, title="DELETE STATEMENT", subtitle="DELETE STATEMENT"))


if __name__ == "__main__":
    main()
